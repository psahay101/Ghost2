# -*- coding: utf-8 -*-
"""gpuvscpu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Htjh9W3lkaKvdFSAigVT6pxkvvsaiFuj
"""

import tensorflow as tf

import time
import numpy as np

x = 1000
y = 100

matrix = tf.random.uniform(
    (x,y), minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None
)

start = time.time()
result = np.argmax(tf.reduce_sum(matrix, 0))
end = time.time()

(end - start), result

matrix1 = np.random.rand(x,y)

cpu_start = time.time()
result1 = np.argmax(np.sum(matrix1,axis = 0))
cpu_end = time.time()

(cpu_end - cpu_start), result1

####### when the dimensionality is 10000X1000 gpu took 0.0045 seconds where as cpu took 0.012
####### When the dimensinality is 1000X100 gpu took 0.002 secs where as cpu took 0.0003 
####### As the dimensionality increases above 10000 gpu seems to be more effective